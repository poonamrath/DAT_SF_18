{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stop words, tf-idf\n",
    "\n",
    "Let's investigate one of the most useful feature weightings, and how stop words derive naturally from that. To start, let's load a set of small documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "try:\n",
    "    df = pd.read_csv('../data/nlp_data/rt_critics.csv')\n",
    "except IOError:\n",
    "    print 'cannot find file'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 14072 entries, 0 to 14071\n",
      "Data columns (total 8 columns):\n",
      "critic         13382 non-null object\n",
      "fresh          14072 non-null object\n",
      "imdb           14072 non-null float64\n",
      "publication    14072 non-null object\n",
      "quote          14072 non-null object\n",
      "review_date    14072 non-null object\n",
      "rtid           14072 non-null float64\n",
      "title          14072 non-null object\n",
      "dtypes: float64(2), object(6)\n",
      "memory usage: 989.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>critic</th>\n",
       "      <th>fresh</th>\n",
       "      <th>imdb</th>\n",
       "      <th>publication</th>\n",
       "      <th>quote</th>\n",
       "      <th>review_date</th>\n",
       "      <th>rtid</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Derek Adams</td>\n",
       "      <td>fresh</td>\n",
       "      <td>114709</td>\n",
       "      <td>Time Out</td>\n",
       "      <td>So ingenious in concept, design and execution ...</td>\n",
       "      <td>2009-10-04</td>\n",
       "      <td>9559</td>\n",
       "      <td>Toy story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Richard Corliss</td>\n",
       "      <td>fresh</td>\n",
       "      <td>114709</td>\n",
       "      <td>TIME Magazine</td>\n",
       "      <td>The year's most inventive comedy.</td>\n",
       "      <td>2008-08-31</td>\n",
       "      <td>9559</td>\n",
       "      <td>Toy story</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            critic  fresh    imdb    publication  \\\n",
       "0      Derek Adams  fresh  114709       Time Out   \n",
       "1  Richard Corliss  fresh  114709  TIME Magazine   \n",
       "\n",
       "                                               quote review_date  rtid  \\\n",
       "0  So ingenious in concept, design and execution ...  2009-10-04  9559   \n",
       "1                  The year's most inventive comedy.  2008-08-31  9559   \n",
       "\n",
       "       title  \n",
       "0  Toy story  \n",
       "1  Toy story  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['So ingenious in concept, design and execution that you could watch it on a postage stamp-sized screen and still be engulfed by its charm.',\n",
       " \"The year's most inventive comedy.\",\n",
       " 'A winning animated feature that has something for everyone on the age spectrum.',\n",
       " \"The film sports a provocative and appealing story that's every bit the equal of this technical achievement.\",\n",
       " \"An entertaining computer-generated, hyperrealist animation feature (1995) that's also in effect a toy catalog.\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It seems silly to call such short blurbs 'documents', but we'll stick with the NLP nomenclature.\n",
    "\n",
    "documents = list(df['quote'])\n",
    "documents[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Frequency\n",
    "\n",
    "Let's start by calculating the document frequency for words in these documents. For this task, let's also remove all the punctuation marks and make everything lower-case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize  # for tokenizing our text\n",
    "import string  # helps with removing punctuation\n",
    "from collections import Counter  # great dict-like datastructure for counting things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "print string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a few tokens: [['so', 'ingenious', 'in', 'concept', 'design', 'and', 'execution', 'that', 'you', 'could', 'watch', 'it', 'on', 'a', 'postage', 'stamp', 'sized', 'screen', 'and', 'still', 'be', 'engulfed', 'by', 'its', 'charm'], ['the', 'years', 'most', 'inventive', 'comedy'], ['a', 'winning', 'animated', 'feature', 'that', 'has', 'something', 'for', 'everyone', 'on', 'the', 'age', 'spectrum']]\n",
      "number of tokens: 280092\n",
      "number of unique tokens: 22424\n",
      "number of documents: 14072\n"
     ]
    }
   ],
   "source": [
    "# This is a bit of text cleanup\n",
    "word_bag_list = []\n",
    "for doc in documents:\n",
    "    cleaned = doc.lower().replace('-', ' ')  # make lowercase and split hyphenated words in two\n",
    "    for c in string.punctuation:  # strip punctuation marks.\n",
    "        cleaned = cleaned.replace(c, '')\n",
    "    word_bag_list.append(wordpunct_tokenize(cleaned))\n",
    "\n",
    "# How do things look?\n",
    "print 'a few tokens:', word_bag_list[:3]\n",
    "\n",
    "# this flattens the nested lists into one big list for some stats\n",
    "token_list = []\n",
    "for tokens in word_bag_list:\n",
    "    token_list.extend(tokens)\n",
    "print 'number of tokens:', len(token_list)\n",
    "print 'number of unique tokens:', len(set(token_list))\n",
    "print 'number of documents:', len(word_bag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-dd287ec59440>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_bag_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mword_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "word_dict = {}\n",
    "count = 0\n",
    "for item in word_bag_list:\n",
    "    if item not in word_dict:\n",
    "        word_dict[item] = count+1\n",
    "    print word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = [7,7,2,2,3,3,3]\n",
    "count_dict ={}\n",
    "count = 0\n",
    "def return_count(input_list):\n",
    "    for item in a:\n",
    "        if item not in count_dict:\n",
    "            count_dict[item]= count+1\n",
    "        else:\n",
    "            count_dict[item] = 1\n",
    "    return input   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function ipykernel.ipkernel.<lambda>>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_count(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 0.6140562819783968),\n",
       " ('a', 0.5035531552018192),\n",
       " ('and', 0.48969584991472426),\n",
       " ('of', 0.4640420693575895),\n",
       " ('is', 0.3320068220579875),\n",
       " ('to', 0.32106310403638433),\n",
       " ('in', 0.23848777714610575),\n",
       " ('that', 0.20082433200682206),\n",
       " ('its', 0.1991898806139852),\n",
       " ('it', 0.1960631040363843),\n",
       " ('with', 0.15513075611142696),\n",
       " ('but', 0.15157760090960773),\n",
       " ('this', 0.1467453098351336),\n",
       " ('movie', 0.12933484934621944),\n",
       " ('film', 0.12926378624218307),\n",
       " ('for', 0.1286242183058556),\n",
       " ('as', 0.12784252416145536),\n",
       " ('an', 0.10993462194428652),\n",
       " ('be', 0.08484934621944286),\n",
       " ('on', 0.08449403069926094)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the document frequency of all the unique tokens in the bags of words.\n",
    "\n",
    "df = Counter()  # initialize this dict-like thing.\n",
    "\n",
    "for doc in word_bag_list:\n",
    "    \n",
    "    # FILL IN CODE\n",
    "    # count up the times words appear in INDIVIDUAL documents (not the total across all documents)\n",
    "    for token in set(doc):  # edit this, obviously\n",
    "        # add one to the right key in df\n",
    "        df[token]+=1\n",
    "for token in df:\n",
    "    df[token] =df[token]/float(len(documents))\n",
    "    # normalize the counts by the number of documents (are you getting zeros? Think datatypes.)\n",
    "\n",
    "# this last line prints the 20 highest-scoring words and their scores\n",
    "df.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words\n",
    "\n",
    "Which words are likely to be stop words? The ones that show up in the most documents! These terms with the largest document frequency are the stopwords! The threshold above which you call something a stopword is up to you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf-idf\n",
    "\n",
    "More interesting than stop-words is the tf-idf score. This tells us which words are most discriminative between documents. Words that occur a lot in one document but doesn't occur in many documents will tell you something special about the document:\n",
    "\n",
    "$$\n",
    "\\text{tf-idf} = tf \\cdot \\log{idf} = tf \\cdot \\log{1 \\over df} = tf \\cdot -\\log{df}\n",
    "$$\n",
    "\n",
    "recall that:\n",
    "\n",
    "$$\n",
    "\\log{x} = -\\log{1 \\over x}\n",
    "$$\n",
    "\n",
    "What are the most discriminative words in the first few documents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('engulfed', 0.39799759526739137), ('postage', 0.3691164627440604), ('sized', 0.34023533022072933), ('stamp', 0.31691800572341999), ('ingenious', 0.27994703926504905)]\n",
      "[('inventive', 1.1776761280575496), ('years', 0.8588893828779226), ('comedy', 0.65543605303509112), ('most', 0.59453821488145864), ('the', 0.097533738115198984)]\n",
      "[('spectrum', 0.65025615367302192), ('winning', 0.47574203511007074), ('everyone', 0.43231666566869759), ('age', 0.39485397527852278), ('animated', 0.39393272981339084)]\n",
      "[('equal', 0.41634815803257685), ('sports', 0.39581665384682457), ('provocative', 0.36964725791818803), ('technical', 0.36339204175837664), ('achievement', 0.36339204175837664)]\n",
      "[('catalog', 0.63679615242782617), ('hyperrealist', 0.63679615242782617), ('1995', 0.49031451393874498), ('toy', 0.45195690427850749), ('generated', 0.41464918508281268)]\n",
      "[('ushered', 0.26533173017826089), ('revived', 0.26533173017826089), ('lion', 0.21556063381081494), ('repetition', 0.21556063381081494), ('landmark', 0.21127867048228)]\n",
      "[('conceptual', 0.39799759526739137), ('wholl', 0.35222208323955351), ('verbal', 0.32334095071622243), ('defined', 0.30644657121171559), ('appreciated', 0.30644657121171559)]\n",
      "[('anthropomorphism', 0.39799759526739137), ('toys', 0.30644657121171559), ('marvel', 0.28247306517406717), ('irresistible', 0.27531263780212306), ('utterly', 0.24985809270533249)]\n",
      "[('foray', 0.3293773202212894), ('invested', 0.28157406638956911), ('cleverness', 0.25767243947370888), ('imagery', 0.22125683001683602), ('generated', 0.21447371642214447)]\n",
      "[('toys', 0.35022465281338927), ('speak', 0.3291849979429588), ('tim', 0.30987713565209385), ('hanks', 0.30351849790806895), ('guys', 0.29617798934486617)]\n",
      "[('coaster', 0.69869929289558574), ('roller', 0.69869929289558574), ('visionary', 0.67793535641776126), ('ride', 0.56601219883067677), ('result', 0.52614828452690032)]\n",
      "[('wondrously', 0.40828239626487522), ('holiday', 0.33037516536254774), ('generated', 0.31098688881210951), ('imaginative', 0.30431031918088336), ('town', 0.30431031918088336)]\n",
      "[('popper', 0.70444416647910701), ('d', 0.53470400587402034), ('3', 0.50718386530147219), ('disneys', 0.49508536448109741), ('eye', 0.4635798533210933)]\n",
      "[('docter', 0.34114079594347829), ('pete', 0.34114079594347829), ('ranft', 0.34114079594347829), ('throats', 0.31638553949490889), ('lasseter', 0.31638553949490889)]\n",
      "[('overcommercialization', 0.95519422864173942), ('negative', 0.84533299977492848), ('involves', 0.73547177090811755), ('toy', 0.67793535641776126), ('disneys', 0.59410243737731694)]\n",
      "[('technically', 1.1521474928003559), ('toy', 1.1298922606962687), ('flawless', 1.0768166388431797), ('nearly', 0.87921269456688977), ('story', 0.53005507319758949)]\n",
      "[('playthings', 0.35377564023768121), ('eager', 0.28741417841441996), ('andy', 0.27239695218819165), ('draw', 0.27239695218819165), ('20th', 0.26496470420811197)]\n",
      "[('miraculous', 0.28177766659164283), ('toy', 0.22597845213925374), ('imagine', 0.20287354612058892), ('produced', 0.199886474164266), ('hardly', 0.19894744493204281)]\n",
      "[('changer', 0.68228159188695658), ('puns', 0.60380928555352031), ('added', 0.48423954029840088), ('voice', 0.4417604611736371), ('game', 0.38088818437437766)]\n",
      "[('gloomy', 0.61096187492179188), ('generating', 0.57480774959519676), ('grotesque', 0.56574751608316731), ('extravaganza', 0.56574751608316731), ('despair', 0.53746099453506591)]\n",
      "[('extravaganza', 0.91933971363514688), ('calculated', 0.86411061960026692), ('special', 0.56683280620030874), ('effects', 0.53745860729634554), ('entertaining', 0.51587403361699891)]\n",
      "[('matthau', 0.68144577737364997), ('walter', 0.5850793951816986), ('lemmon', 0.57480774959519676), ('awfully', 0.53176038129247194), ('jack', 0.48715895858070718)]\n",
      "[('regrettably', 3.8800914085946694), ('mediocre', 3.2781050064317014)]\n",
      "[('bickering', 0.49725470574995789), ('neil', 0.45648134218760816), ('simon', 0.43263045347536322), ('disappointed', 0.41570797862525849), ('expect', 0.33416125150055903)]\n",
      "[('1993', 0.28170489397637333), ('list', 0.22209608240473999), ('10', 0.21808817186250917), ('wont', 0.20338886206929535), ('among', 0.2027557349078398)]\n",
      "[('ariels', 0.39799759526739137), ('grumps', 0.39799759526739137), ('dictated', 0.3691164627440604), ('practiced', 0.3691164627440604), ('progress', 0.31691800572341999)]\n",
      "[('grumpys', 0.73476479126287642), ('poke', 0.65025615367302192), ('somewhere', 0.49029911200534226), ('cheap', 0.48715895858070718), ('worthy', 0.46816972181674366)]\n",
      "[('melrose', 0.56187895802455257), ('escapes', 0.48033223089985316), ('aura', 0.43955886733750338), ('queasy', 0.43263045347536322), ('problems', 0.36586692801424647)]\n",
      "[('rigorous', 0.37116581478625021), ('examination', 0.3343053504127807), ('undemanding', 0.3343053504127807), ('diverting', 0.30279866038732861), ('sit', 0.28436842820059388)]\n",
      "[('flaring', 0.43417919483715423), ('rejoice', 0.43417919483715423), ('lollygags', 0.43417919483715423), ('stomp', 0.43417919483715423), ('sputtering', 0.37116581478625021)]\n",
      "[('celebratory', 0.73476479126287642), ('mcmillans', 0.73476479126287642), ('sister', 0.62812676348442342), ('terry', 0.61096187492179188), ('selling', 0.51682530325855214)]\n",
      "[('crippling', 0.31638553949490889), ('bashing', 0.31638553949490889), ('exhale', 0.30190464277676016), ('crack', 0.28366087049940331), ('psyche', 0.27714938632819064)]\n",
      "[('mishandles', 0.73476479126287642), ('whitaker', 0.65025615367302192), ('clumsily', 0.59693713978379537), ('tour', 0.52148873570597021), ('girl', 0.44910540151639128)]\n",
      "[('grappling', 0.36738239563143821), ('exhale', 0.32512807683651096), ('waiting', 0.25028537879592194), ('issues', 0.23787101755503537), ('uneven', 0.2218366404807359)]\n",
      "[('bernadine', 0.41530183853988667), ('populating', 0.41530183853988667), ('exception', 0.31518944319231951), ('capture', 0.30726241898388668), ('sympathy', 0.28963350124005344)]\n",
      "[('musing', 0.38207769145669573), ('wardrobe', 0.35435180423429802), ('escapist', 0.26874915769444713), ('figure', 0.25024421681652259), ('problems', 0.24878951104968761)]\n",
      "[('fraker', 0.29849819645054354), ('equilibriums', 0.29849819645054354), ('rattled', 0.26416656242966513), ('retro', 0.23768850429256502), ('fluff', 0.23768850429256502)]\n",
      "[('crook', 0.25816060233560523), ('somebodys', 0.25816060233560523), ('jobs', 0.23942689475290405), ('bruised', 0.23942689475290405), ('stare', 0.22069318717020284)]\n",
      "[('warring', 0.34114079594347829), ('taciturn', 0.34114079594347829), ('braying', 0.34114079594347829), ('ashley', 0.30190464277676016), ('val', 0.29163028304633942)]\n",
      "[('robberies', 0.39799759526739137), ('tautly', 0.3691164627440604), ('elaborately', 0.31691800572341999), ('satisfaction', 0.30644657121171559), ('edited', 0.29112470537316071)]\n",
      "[('reinvests', 0.28945279655810285), ('modernist', 0.25616151508331164), ('postmodernist', 0.25616151508331164), ('artifice', 0.2474438765241668), ('lay', 0.23048582234430548)]\n",
      "[('proportion', 0.26533173017826089), ('nowadays', 0.24607764182937358), ('lengthy', 0.22682355348048622), ('robbers', 0.22062512149953592), ('1995', 0.20429771414114373)]\n",
      "[('asphalt', 0.36738239563143821), ('interpersonal', 0.36738239563143821), ('discussions', 0.34072288868682499), ('fitted', 0.34072288868682499), ('robbers', 0.30548093746089594)]\n",
      "[('invited', 0.35222208323955351), ('fortunately', 0.31135419769739825), ('mann', 0.29445981819289141), ('exceptional', 0.28803687320008897), ('party', 0.27317541720264177)]\n",
      "[('forges', 0.63277107898981777), ('180', 0.58326056609267884), ('heat', 0.46481570347814072), ('mark', 0.44956041059970803), ('consistently', 0.4347290274012619)]\n",
      "[('occupies', 0.76848454524993492), ('exalted', 0.74233162957250043), ('position', 0.72204585218029949), ('countless', 0.70547116519903086), ('heat', 0.5915836226085428)]\n",
      "[('niros', 0.59693713978379537), ('devoted', 0.56574751608316731), ('parody', 0.49357292849909573), ('robert', 0.41981520955348417), ('himself', 0.40163145739469719)]\n",
      "[('decade', 0.33770779318359179), ('indeed', 0.33770779318359179), ('finest', 0.30601435095442242), ('crime', 0.29278727578174313), ('period', 0.28724198548396279)]\n",
      "[('beside', 0.35502817066510889), ('expose', 0.33739925292127559), ('weaknesses', 0.33069704945052525), ('mouse', 0.32489133672771991), ('pale', 0.32489133672771991)]\n",
      "[('superlative', 0.28366087049940331), ('rife', 0.2668750265977699), ('meticulously', 0.26266848961004197), ('controlled', 0.25890561405083384), ('mann', 0.25239412987962123)]\n",
      "[('tires', 0.49215528365874717), ('squealing', 0.49215528365874717), ('banalities', 0.45364710696097243), ('drowning', 0.43112126762162989), ('bodies', 0.41513893026319765)]\n",
      "[('elevate', 0.60380928555352031), ('manns', 0.60380928555352031), ('writing', 0.41198233232136661), ('michael', 0.39224994827649817), ('material', 0.33405321165830298)]\n",
      "[('colossal', 2.8177766659164281), ('disappointment', 2.1388160234960814), ('a', 0.2286886669973256)]\n",
      "[('generates', 0.72493571934233492), ('heat', 0.65074198486939716), ('lots', 0.62197377762421902), ('energy', 0.50860341677628107), ('light', 0.50746054719391875)]\n",
      "[('touts', 0.30812717052959332), ('boosters', 0.30812717052959332), ('yawns', 0.2857675840599177), ('basics', 0.2857675840599177), ('term', 0.22796889150417401)]\n",
      "[('watertight', 0.53066346035652179), ('pacino', 0.37326271902006541), ('niro', 0.37008725158451272), ('each', 0.29875305647343092), ('acted', 0.28791635744168736)]\n",
      "[('bulk', 0.59699639290108708), ('cop', 0.40671374054337317), ('crime', 0.34768488999081998), ('thought', 0.33057890058600492), ('stuff', 0.32081885491379974)]\n",
      "[('glittering', 0.38884037739511923), ('measure', 0.3228263601989339), ('somehow', 0.27687012705400121), ('lead', 0.27574962717732526), ('formula', 0.27253813761563328)]\n",
      "[('dropped', 0.3293773202212894), ('glibly', 0.3293773202212894), ('quipping', 0.3293773202212894), ('tugs', 0.3293773202212894), ('trouping', 0.30547569330542929)]\n",
      "[('tastelessness', 1.1073493882321812), ('wilders', 0.89425587670237794), ('artistic', 0.78201317755163313), ('strength', 0.76884311309440478), ('major', 0.69082382446028057)]\n",
      "[('callousness', 0.39799759526739137), ('barriers', 0.35222208323955351), ('confrontation', 0.31135419769739825), ('breaking', 0.29445981819289141), ('cinderella', 0.29112470537316071)]\n",
      "[('hampered', 0.42977094343671074), ('unimaginative', 0.38709040574111447), ('delightfully', 0.37652879019047492), ('cinderella', 0.36773646994504511), ('variation', 0.36773646994504511)]\n",
      "[('wilderhas', 0.3293773202212894), ('column', 0.3293773202212894), ('screed', 0.30547569330542929), ('yesterdays', 0.2914941378534236), ('feather', 0.28157406638956911)]\n",
      "[('lump', 2.7218826417658346), ('joyless', 2.4515725696937247), ('a', 0.2286886669973256)]\n",
      "[('reuniting', 0.43417919483715423), ('abilities', 0.38424227262496746), ('timecop', 0.37116581478625021), ('biter', 0.36102292609014974), ('exploits', 0.33965912476079813)]\n",
      "[('mounts', 0.5536746941160906), ('steadily', 0.52833312485933026), ('pyrotechnics', 0.43668705805974106), ('combat', 0.41992055889757363), ('count', 0.40380623956619238)]\n",
      "[('claude', 0.31770017495933173), ('sudden', 0.29418870836324701), ('1995', 0.29418870836324701), ('building', 0.28997428773693396), ('damme', 0.26874915769444713)]\n",
      "[('puck', 0.29849819645054354), ('goalie', 0.29849819645054354), ('booby', 0.29849819645054354), ('jollies', 0.29849819645054354), ('jeopardy', 0.2768373470580453)]\n",
      "[('dropping', 0.34532627712970843), ('claude', 0.34532627712970843), ('jaw', 0.33069704945052525), ('damme', 0.29211864966787732), ('loaded', 0.28505260925493053)]\n",
      "[('manipulation', 0.31691800572341999), ('sudden', 0.30644657121171559), ('common', 0.26920415971079492), ('create', 0.24985809270533249), ('death', 0.23583841617944862)]\n",
      "[('patently', 0.36738239563143821), ('atop', 0.36738239563143821), ('roof', 0.34072288868682499), ('overtime', 0.32512807683651096), ('arena', 0.31406338174221171)]\n",
      "[('thwart', 0.34114079594347829), ('chores', 0.34114079594347829), ('defuse', 0.34114079594347829), ('boothe', 0.34114079594347829), ('bombs', 0.34114079594347829)]\n",
      "[('connecting', 0.32512807683651096), ('worthless', 0.31406338174221171), ('choreographed', 0.2925396975908493), ('hyams', 0.27515565436996248), ('flair', 0.25841265162927607)]\n",
      "[('brosnan', 0.35377564023768121), ('handicap', 0.3281035224391648), ('recapture', 0.30243140464064827), ('humorless', 0.27675928684213175), ('amused', 0.27675928684213175)]\n",
      "[('onatopp', 0.34114079594347829), ('xenia', 0.34114079594347829), ('domination', 0.31638553949490889), ('fighting', 0.28366087049940331), ('returns', 0.25890561405083384)]\n",
      "[('clinging', 0.34114079594347829), ('mite', 0.31638553949490889), ('entries', 0.29163028304633942), ('cycle', 0.28366087049940331), ('deny', 0.27714938632819064)]\n",
      "[('kiel', 1.9103884572834788), ('missed', 1.332314105704246), ('richard', 1.199318844985596), ('you', 0.56538172885011029), ('are', 0.53492924298399314)]\n",
      "[('33', 0.25816060233560523), ('breathes', 0.22069318717020284), ('17', 0.209734670734847), ('007', 0.20195947958750157), ('bonds', 0.19877615429949122)]\n",
      "[('bowling', 0.52110559446220295), ('recapture', 0.48033223089985316), ('campbell', 0.4672061396460761), ('matters', 0.42082629491876611), ('earlier', 0.35274083676046941)]\n",
      "[('blahs', 3.1839807621391309), ('suffers', 2.1388160234960814), ('the', 0.16255623019199827)]\n",
      "[('thundering', 1.2655421579796355), ('supercharged', 1.2076185711070406), ('numbing', 0.98755499382887646), ('brain', 0.90472378022131328), ('spectacular', 0.83405288853044079)]\n",
      "[('souviens', 0.43417919483715423), ('je', 0.43417919483715423), ('plate', 0.38424227262496746), ('vanity', 0.3343053504127807), ('read', 0.2916567304767384)]\n",
      "[('facsimile', 0.45485439459130439), ('weariness', 0.42184738599321187), ('unfold', 0.35583336879702654), ('reasonable', 0.34520748540111179), ('certain', 0.25607404840579795)]\n",
      "[('goldeneye', 0.98431056731749433), ('babes', 0.98431056731749433), ('bs', 0.98431056731749433), ('excels', 0.93925888863880935), ('stunts', 0.67282608055010151)]\n",
      "[('proficiency', 1.0613269207130436), ('dollar', 0.90729421392194487), ('wit', 0.55890596770759482), ('top', 0.55534059942409475), ('directed', 0.5171224984962759)]\n",
      "[('spy', 0.6107935402146526), ('loved', 0.60068211884099587), ('entry', 0.5915836226085428), ('perhaps', 0.47346698768761003), ('me', 0.45728670085166856)]\n",
      "[('belated', 0.35435180423429802), ('observed', 0.29418870836324701), ('bonds', 0.29418870836324701), ('satisfaction', 0.29418870836324701), ('neat', 0.28268142546517577)]\n",
      "[('sweats', 0.63679615242782617), ('exudes', 0.56355533318328566), ('capra', 0.46085899712014233), ('corny', 0.45625947235434555), ('sympathy', 0.44410470190141532)]\n",
      "[('oval', 0.28945279655810285), ('sorkin', 0.28945279655810285), ('zinger', 0.26844833654113487), ('dandy', 0.26844833654113487), ('twinkling', 0.26844833654113487)]\n",
      "[('notably', 1.0095765195184849), ('genial', 0.98755499382887646), ('inspired', 0.82724888153197584), ('entertaining', 0.58957032413371302), ('if', 0.44765390721159748)]\n",
      "[('coasts', 0.63679615242782617), ('middlebrow', 0.52950029159888623), ('genial', 0.46085899712014233), ('fare', 0.40786367212881652), ('stars', 0.34896361152540561)]\n",
      "[('bustling', 0.93925888863880935), ('impassioned', 0.86224253524325978), ('sturges', 0.78522618184771042), ('capra', 0.76809832853357052), ('best', 0.37887832009222638)]\n",
      "[('unwatchable', 0.51035299533109402), ('reiner', 0.43668705805974106), ('president', 0.43668705805974106), ('rob', 0.41992055889757363), ('nevertheless', 0.40102800440551528)]\n",
      "[('swept', 0.57480774959519676), ('dislike', 0.57480774959519676), ('wont', 0.42242302122084424), ('either', 0.41365807819398137), ('away', 0.38862558585285611)]\n",
      "[('vote', 0.47537700858513005), ('president', 0.43668705805974106), ('change', 0.39581665384682457), ('smile', 0.38873361101513687), ('wont', 0.34321870474193594)]\n",
      "[('bias', 0.44293975529287249), ('abundance', 0.38800914085946697), ('smarts', 0.38030160686810405), ('spare', 0.36773588545405878), ('president', 0.34934964644779287)]\n",
      "[('entertainment', 2.1794927177635919), ('great', 1.9307914160466668)]\n",
      "[('modulated', 2.0414119813243761), ('charmer', 2.0414119813243761), ('well', 0.88272973426696688), ('a', 0.1715165002479942)]\n",
      "[('simpering', 0.56187895802455257), ('darn', 0.43955886733750338), ('president', 0.41099958405622689), ('20', 0.37493461506290876), ('affecting', 0.36800620120076855)]\n",
      "[('frothy', 7.9425043739832937)]\n"
     ]
    }
   ],
   "source": [
    "# calculate the term frequency of all the unique tokens in all of the bags of words.\n",
    "\n",
    "for doc in word_bag_list[:100]:\n",
    "    tf = Counter()  # initialize this dict-like thing. # its a class that makes it easy to do things like getmost common\n",
    "    tfidf = Counter()\n",
    "    \n",
    "    # FILL IN CODE\n",
    "\n",
    "    # calculate term frequencies\n",
    "    # this is similar to the document frequencies.\n",
    "    for token in set(doc):\n",
    "        tf[token]+=1 \n",
    "    total = float(sum(tf.values()))\n",
    "\n",
    "#     for token in tf: ## term frequency\n",
    "#         tf[token] =tf[token]/float(len(doc))\n",
    "\n",
    "    # calculate tf-idf scores\n",
    "    for token in tf:\n",
    "        tfidf[token] = (tf[token]/total)*(-np.log(df[token])) # fill this in. you can use np.log().\n",
    "\n",
    "    # this prints most significant words in the document\n",
    "    print tfidf.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sci-Kit Learn\n",
    "\n",
    "Scikit-Learn comes with utilities to do these calculations for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21254"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output.toarray()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.33171187  0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "tfidf_vec = TfidfVectorizer(stop_words='english')\n",
    "output = tfidf_vec.fit_transform(documents)\n",
    "print output.toarray()[20:30, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset(['all', 'six', 'less', 'being', 'indeed', 'over', 'move', 'anyway', 'four', 'not', 'own', 'through', 'yourselves', 'fify', 'where', 'mill', 'only', 'find', 'before', 'one', 'whose', 'system', 'how', 'somewhere', 'with', 'thick', 'show', 'had', 'enough', 'should', 'to', 'must', 'whom', 'seeming', 'under', 'ours', 'has', 'might', 'thereafter', 'latterly', 'do', 'them', 'his', 'around', 'than', 'get', 'very', 'de', 'none', 'cannot', 'every', 'whether', 'they', 'front', 'during', 'thus', 'now', 'him', 'nor', 'name', 'several', 'hereafter', 'always', 'who', 'cry', 'whither', 'this', 'someone', 'either', 'each', 'become', 'thereupon', 'sometime', 'side', 'two', 'therein', 'twelve', 'because', 'often', 'ten', 'our', 'eg', 'some', 'back', 'up', 'go', 'namely', 'towards', 'are', 'further', 'beyond', 'ourselves', 'yet', 'out', 'even', 'will', 'what', 'still', 'for', 'bottom', 'mine', 'since', 'please', 'forty', 'per', 'its', 'everything', 'behind', 'un', 'above', 'between', 'it', 'neither', 'seemed', 'ever', 'across', 'she', 'somehow', 'be', 'we', 'full', 'never', 'sixty', 'however', 'here', 'otherwise', 'were', 'whereupon', 'nowhere', 'although', 'found', 'alone', 're', 'along', 'fifteen', 'by', 'both', 'about', 'last', 'would', 'anything', 'via', 'many', 'could', 'thence', 'put', 'against', 'keep', 'etc', 'amount', 'became', 'ltd', 'hence', 'onto', 'or', 'con', 'among', 'already', 'co', 'afterwards', 'formerly', 'within', 'seems', 'into', 'others', 'while', 'whatever', 'except', 'down', 'hers', 'everyone', 'done', 'least', 'another', 'whoever', 'moreover', 'couldnt', 'throughout', 'anyhow', 'yourself', 'three', 'from', 'her', 'few', 'together', 'top', 'there', 'due', 'been', 'next', 'anyone', 'eleven', 'much', 'call', 'therefore', 'interest', 'then', 'thru', 'themselves', 'hundred', 'was', 'sincere', 'empty', 'more', 'himself', 'elsewhere', 'mostly', 'on', 'fire', 'am', 'becoming', 'hereby', 'amongst', 'else', 'part', 'everywhere', 'too', 'herself', 'former', 'those', 'he', 'me', 'myself', 'made', 'twenty', 'these', 'bill', 'cant', 'us', 'until', 'besides', 'nevertheless', 'below', 'anywhere', 'nine', 'can', 'of', 'your', 'toward', 'my', 'something', 'and', 'whereafter', 'whenever', 'give', 'almost', 'wherever', 'is', 'describe', 'beforehand', 'herein', 'an', 'as', 'itself', 'at', 'have', 'in', 'seem', 'whence', 'ie', 'any', 'fill', 'again', 'hasnt', 'inc', 'thereby', 'thin', 'no', 'perhaps', 'latter', 'meanwhile', 'when', 'detail', 'same', 'wherein', 'beside', 'also', 'that', 'other', 'take', 'which', 'becomes', 'you', 'if', 'nobody', 'see', 'though', 'may', 'after', 'upon', 'most', 'hereupon', 'eight', 'but', 'serious', 'nothing', 'such', 'why', 'a', 'off', 'whereby', 'third', 'i', 'whole', 'noone', 'sometimes', 'well', 'amoungst', 'yours', 'their', 'rather', 'without', 'so', 'five', 'the', 'first', 'whereas', 'once'])\n"
     ]
    }
   ],
   "source": [
    "print tfidf_vec.get_stop_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tfidf_vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
