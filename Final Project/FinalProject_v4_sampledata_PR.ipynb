{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Source:\n",
    "Kaggle (ongoing competition): https://www.kaggle.com/c/prudential-life-insurance-assessment\n",
    "\n",
    "# Project goal:\n",
    "Analyze features collected from individuals applying for life insurance at Prudential to predict response of the company.\n",
    "\n",
    "**Motivation for the problem:**\n",
    "Once Prudential collects features from its applicants, it takes a long time to come up with a decision. If we can come up with a model that accurately predicts the decisions they come up with, the company can use this model and swiftly determine its decision.\n",
    "\n",
    "# Notes on dataset:\n",
    "59381 rows, 127 columns: 126 features (some of which are dummy variables) and 1 outcome column. The outcome column is called **Response**. There are 8 nominal values (1,2,3,4,5,6,7,8) in the Response column.\n",
    "\n",
    "# Immediate goal: working with a smaller subset of Prudential data \n",
    "\n",
    "The point is to be able to go through with a workflow on my laptop. Using the entire datasets chokes Jupyter.\n",
    "- Plan to use Amazon's EC2 cloud server but for now will work with a subset sampled from the data.\n",
    "- Data set (59381 rows) is randomly subsetted to a smaller df with 500 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports and magic here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.manifold import Isomap\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.learning_curve import learning_curve\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB,GaussianNB\n",
    "from sklearn.cross_validation import train_test_split, StratifiedKFold, ShuffleSplit \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Globals here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "File ./train.csv does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-50ecc2f9429a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Read input dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0minput_csv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./train.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_csv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/PoonamRath/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, dialect, compression, doublequote, escapechar, quotechar, quoting, skipinitialspace, lineterminator, header, index_col, names, prefix, skiprows, skipfooter, skip_footer, na_values, true_values, false_values, delimiter, converters, dtype, usecols, engine, delim_whitespace, as_recarray, na_filter, compact_ints, use_unsigned, low_memory, buffer_lines, warn_bad_lines, error_bad_lines, keep_default_na, thousands, comment, decimal, parse_dates, keep_date_col, dayfirst, date_parser, memory_map, float_precision, nrows, iterator, chunksize, verbose, encoding, squeeze, mangle_dupe_cols, tupleize_cols, infer_datetime_format, skip_blank_lines)\u001b[0m\n\u001b[1;32m    489\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/PoonamRath/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/PoonamRath/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    581\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_options_with_defaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/PoonamRath/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    722\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    725\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/PoonamRath/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1091\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1093\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:3229)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas/parser.c:6042)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: File ./train.csv does not exist"
     ]
    }
   ],
   "source": [
    "# Read input dataset\n",
    "input_csv = './train.csv'\n",
    "df_raw = pd.read_csv(input_csv, index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Randomly extract 500 rows from df_raw into df_sampled **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sample df_raw to create a smaller, easier to work with df\n",
    "rows = np.random.choice(df_raw.index.values,500)\n",
    "df_sampled = df_raw.ix[rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_sampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Identify predictor variables and response varibales, and create corresponding DFs\n",
    "\n",
    "def split_response_from_features(df):\n",
    "    response = 'Response'\n",
    "    all_predictors = [col for col in df.columns if col not in response]\n",
    "    df_predictors = df[all_predictors]\n",
    "    df_response = df[response]\n",
    "    df_response_with_dummies = pd.get_dummies(df_response,prefix='Response')\n",
    "    return df_predictors, df_response, df_response_with_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_predictors, df_response_with_dummies, df_response = split_response_from_features(df_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_sampled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_response.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preliminary exploratory plots:**\n",
    "1. Wt versus BMI (Intuitively one would guess there is a positive correlation).\n",
    "2. Plot the distribution of Response variable to see if there are evenly represented in df_sample. If not we might need to upsample, downsample or generate rows for the underrepresented response using SMOTE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot scatterplot of Wt vs BMI\n",
    "with plt.style.context('fivethirtyeight'):\n",
    "    plt.scatter(df_sampled.BMI,df_sampled.Wt,s=40)\n",
    "    # add axis label\n",
    "    plt.xlabel('BMI')\n",
    "    plt.ylabel('Wt')\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the distribution of Response\n",
    "sns.distplot(df_sampled.Response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.countplot(df_sampled.Response, palettle=\"husl\", order=range(1,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "response_count = {}\n",
    "for ii in df_sampled.Response.unique():\n",
    "    response_count[ii] = len(df_sampled[df_sampled.Response==ii])\n",
    "    total_num = len(df_sampled[df_sampled.Response==ii])\n",
    "    print \"Total number of Response#{} : {}\".format(ii,total_num)\n",
    "print response_count       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on the Response column\n",
    "1. Response 6,8 is roughly 4-5 times higher than the rest of the responses : consider undersampling.\n",
    "2. Response 3 and 4 are way less than the others: consider oversampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Describe each feature: Number of uniques, number of nulls, type_of_feature, distribution\n",
    "\n",
    "def get_dummy_features(df, verbose=False, dummy_threshold = 200):\n",
    "    features_for_dummification = []\n",
    "    num_rows = len(df)\n",
    "\n",
    "    for each_feature in df.columns:\n",
    "        num_uniques = len(df[each_feature].unique())\n",
    "        num_nulls = df[each_feature].isnull().sum()\n",
    "        example = df[each_feature].iloc[0]\n",
    "        \n",
    "        # Keep track of dummy features\n",
    "        if isinstance(example, str) or (num_uniques*dummy_threshold<num_rows and isinstance(example, (int, long))): \n",
    "            features_for_dummification.append(each_feature)\n",
    "\n",
    "        if verbose==True:\n",
    "            print '{}: Uniques: {}/{}. Nulls: {}. Type: {}'.format(each_feature, num_uniques, \n",
    "                                                               num_rows, num_nulls, type_of_feature)\n",
    "            \n",
    "            \n",
    "    return features_for_dummification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check if number of dummiable columns in the original df and the sampled df are the same\n",
    "print len(get_dummy_features(df_raw,verbose=False,dummy_threshold=200))\n",
    "print len(get_dummy_features(df_sampled,verbose=False,dummy_threshold=200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get_dummy_features(df_predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The answer is no: 108 for original df_raw and 94 for df_sampled. But that's ok. Just go ahead with the workflow anyway.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Turn chosen features into dummy variables\n",
    "def create_dummy_features(df, features_for_dummification, verbose=True):\n",
    "    df_expanded = df\n",
    "\n",
    "    for each_feature in features_for_dummification:\n",
    "        if verbose==True:\n",
    "            print \"Expanding variable: {}\".format(each_feature)\n",
    "        df_temp = pd.get_dummies(df[each_feature], prefix=each_feature)\n",
    "        df_expanded = pd.concat([df_expanded, df_temp], axis = 1, join = 'inner')\n",
    "        df_expanded.drop(each_feature,inplace=True, axis=1)\n",
    "        \n",
    "    return df_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Since training data and test data have different distributions for \"dummyizable\" features, it makes sense to\n",
    "# # build out dummy features using a combined distribution\n",
    "df_oos_sneak_peek = pd.read_csv('./test.csv', index_col = 0)\n",
    "df_predictors_enhanced = pd.concat([df_predictors, df_oos_sneak_peek], axis = 0, join = 'outer')\n",
    "df_predictors_enhanced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Since Medical_Keyword is already dummied, make sure you exclude it in the dummying process!\n",
    "features_for_dummification = get_dummy_features(df_predictors)\n",
    "features_already_dummy = ['Medical_Keyword_{}'.format(num) for num in range(1,49)]\n",
    "features_for_dummification = sorted(list(set(features_for_dummification) - set(features_already_dummy)))\n",
    "print features_for_dummification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run the functions from above (df_expanded has dummy variables)\n",
    "df_sample_expanded_enhanced = create_dummy_features(df_predictors_enhanced,features_for_dummification)\n",
    "df_expanded = df_sample_expanded_enhanced.drop(df_oos_sneak_peek.index)\n",
    "df_expanded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Question about performing correlations: should we test this before making dummies?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Compute pairwise correlations of all predictors\n",
    "# df_predictors = df_expanded\n",
    "# df_predictor_correlations = df_predictors.corr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Describe pairwise correlations\n",
    "# _corr_threshold = 0.5\n",
    "\n",
    "# for pred in all_predictors:\n",
    "#     df_temp = df_predictor_correlations[pred]\n",
    "#     max_corr = df_temp[[idx for idx in df_temp.index if pred not in idx]].max()\n",
    "#     min_corr = df_temp.min()\n",
    "#     if np.isnan(max_corr)==False:\n",
    "#         if abs(min_corr)>max_corr:\n",
    "#             other_pred = df_temp[df_temp==min_corr].index[0]\n",
    "#             mcorr = min_corr\n",
    "#         else:\n",
    "#             other_pred = df_temp[df_temp==max_corr].index[0]\n",
    "#             mcorr = max_corr\n",
    "#         if abs(mcorr)>_corr_threshold:\n",
    "#             print '** ',\n",
    "#         print 'mcorr({})={:.4f} with {}'.format(pred, mcorr, other_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering:\n",
    "- Fill NAs for both the predictord and response with median estimates of the column.\n",
    "- Consider localized imputing in future.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fill NAs with median estimates\n",
    "df_predictors_selected = df_expanded\n",
    "df_response_selected = df_response_with_dummies\n",
    "\n",
    "df_predictors_selected.fillna(df_predictors_selected.median(),inplace=True)\n",
    "df_response_selected.fillna(df_response_selected.median(),inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes on Response variable:**\n",
    "    \n",
    "When I use all 8 dummy columns for response, I get \"bad shape\" error below. I will try using just one Response columns(1 through 8) \n",
    "and try OnevsRest or some method...\n",
    "\n",
    "- Test how separable the resposnes are by plotting a 2D projection of the feature space and coloring by response.\n",
    "- For each type of response, find the most correlated feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Assign predictors to X and response to Y\n",
    "X = df_predictors_selected\n",
    "#y = df_response_selected\n",
    "y = df_response_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print X.shape\n",
    "print y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check how separable the responses are using MDS\n",
    "data_to_cluster = X\n",
    "mds = MDS(n_components=2,random_state=1)\n",
    "mds_transformed_data = mds.fit_transform(data_to_cluster)\n",
    "mds_transformed_data_df = pd.DataFrame(mds_transformed_data, columns= ['MDS1','MDS2'])\n",
    "mds_transformed_data_df.head()\n",
    "colors = y\n",
    "plt.scatter(mds_transformed_data_df['MDS1'].values,mds_transformed_data_df['MDS2'].values,c=colors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Make 11 clusters using KMeans clustering\n",
    "k = 8\n",
    "km = KMeans(k)\n",
    "cluster_info = km.fit(X)\n",
    "\n",
    "# PCA to visulaize clusters\n",
    "pca = PCA(n_components=2,copy=True)\n",
    "transformed_data = pca.fit_transform(data_to_cluster)\n",
    "transformed_data_df = pd.DataFrame(transformed_data, columns= ['PCA1','PCA2'])\n",
    "transformed_data_df.head()\n",
    "colors = cluster_info.labels_\n",
    "plt.scatter(transformed_data_df['PCA1'].values,transformed_data_df['PCA2'].values,c=colors)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check how separable the responses using PCA\n",
    "data_to_cluster = X\n",
    "pca = PCA(n_components=2,copy=True)\n",
    "pca_transformed_data = pca.fit_transform(data_to_cluster)\n",
    "pca_transformed_data_df = pd.DataFrame(pca_transformed_data, columns= ['PCA1','PCA2'])\n",
    "pca_transformed_data_df.head()\n",
    "colors = y\n",
    "plt.scatter(pca_transformed_data_df['PCA1'].values,pca_transformed_data_df['PCA2'].values,c=colors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Compute pairwise correlations of all predictors\n",
    "# df_predictors = df_expanded\n",
    "# df_predictor_correlations = df_predictors.corr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Describe pairwise correlations\n",
    "# _corr_threshold = 0.5\n",
    "all_predictors = df_predictors_selected.columns\n",
    "# for pred in all_predictors:\n",
    "#     df_temp = df_predictor_correlations[pred]\n",
    "#     max_corr = df_temp[[idx for idx in df_temp.index if pred not in idx]].max()\n",
    "#     min_corr = df_temp.min()\n",
    "#     if np.isnan(max_corr)==False:\n",
    "#         if abs(min_corr)>max_corr:\n",
    "#             other_pred = df_temp[df_temp==min_corr].index[0]\n",
    "#             mcorr = min_corr\n",
    "#         else:\n",
    "#             other_pred = df_temp[df_temp==max_corr].index[0]\n",
    "#             mcorr = max_corr\n",
    "#         if abs(mcorr)>_corr_threshold:\n",
    "#             print '** ',\n",
    "#         print 'mcorr({})={:.4f} with {}'.format(pred, mcorr, other_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for response in df_response[df_response.Response_1:\n",
    "   \n",
    "        score = {}\n",
    "        score[response] = \n",
    "        score.append(df_response[response].corr(df_predictors_selected[predictor]))\n",
    "        #print 'corr of response {} with feature {}: {}'.format(response,predictor,(df_response[response].corr(df_predictors_selected[predictor]\n",
    "                                                                                                            # )))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score_result_1 = {}\n",
    "for predictor in X:    \n",
    "    score_result_1[predictor]= X[predictor].corr(df_response['Response_1'])\n",
    "    \n",
    "inverse = [(value,key) for key, value in score_result_1.items() ]\n",
    "print \"Max correlated feature with Result_1: {}\".format(max(inverse)[1])\n",
    "print min(inverse)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pylab import*\n",
    "for ii in df_response:\n",
    "    score_result_ii = {}\n",
    "    for predictor in X:\n",
    "        score_result_ii[predictor] = X[predictor].corr(df_response[ii])\n",
    "    inverse = [(value,key) for key, value in score_result_ii.items()]\n",
    "#     pos = arange(1)\n",
    "#     plt.barh(pos,max(inverse)[0],align='center')\n",
    "#     yticks(pos, (max(inverse)[1]))\n",
    "#     plt.show()\n",
    "    print \"Max positively correlated feature with Result_{}: {}, correlation: {}\".format(ii,max(inverse)[1],max(inverse)[0])\n",
    "    print \"Min negatively correlated feature with Result_{}: {},correlation: {}\".format(ii,min(inverse)[1],min(inverse)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning curve\n",
    "\n",
    "The point is to see if we have enough data. We will do this by determining cross-validated training and test scores for different training set sizes. I'm going to give this a shot with Logistic Regression using all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# estimator = LogisticRegression()\n",
    "# title = \"Learning curve Logistic Regression\"\n",
    "# #cv = cross_validation.ShuffleSplit(digits.data.shape[0], n_iter=100,test_size=0.2, random_state=0)\n",
    "# cv=StratifiedKFold(y,n_folds=5,shuffle=True,random_state=1)\n",
    "# # plot_learning_curve(estimator, title, X, y, ylim=(0.7, 1.01), cv=cv, n_jobs=4)\n",
    "# # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model building:\n",
    "\n",
    "**Make another version of the data where the minority response values are oversampled OR synthetically generated using SMOTE. First use the original dataset and then the altered for each model:[X,y OR X1,y1]**\n",
    "\n",
    "1) Fit following models using all features:\n",
    "    - Logistic Regression\n",
    "    - Naive Bayes\n",
    "    - K-Nearest Neighbor\n",
    "\n",
    "\n",
    "\n",
    "2) Plot learning curve for any (?) model that gives a decent accuracy to see if we have enough data in hand!!\n",
    "\n",
    "\n",
    "\n",
    "3) Fit ensemble models using all features:\n",
    "    - RandomForest\n",
    "    - Gradient Boosting Classifier\n",
    "    - Voting Classifier (combo of models from Step#1: show that base classifiers are better than a coin toss)\n",
    "    \n",
    " \n",
    "**Other points to think about:**\n",
    "\n",
    "1) K-fold cross-validation can be folded into the model: each run will give a score: we can get the median score.\n",
    "\n",
    "\n",
    "2) Feature selection: Regularization (C which is 1/lambda.. C=0.1 means high penalty. Apply Lasso). Find coefficients and discuss the importance. Look for intuitive explanations. \n",
    "2.1) For Logistic Regression and Naive Bayes, we will get probabilities so besides just looking at the median CV score,will it be useful to look at the confusion matrix, find TPR, FPR, plot a ROC curve and determine AUC? Is that of interest to Prudential??\n",
    "\n",
    "\n",
    "3) To simplify the model, if scores aren't great, we can even remove features showing multicollinearity. (Should we be looking at covariance?). Besides plotting a correlation matrix, will PCA help in finding out which features are colinear (since they will be on the same axis)?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit LOGISTIC REGRESSION model and compute the max cross_validation score over various K-folds.\n",
    "model_lr = LogisticRegression(penalty = 'l1',C=0.05) # Instantiate Logistic Regression model\n",
    "scores =[]\n",
    "jj = 0\n",
    "for ii in range(2,11):\n",
    "    scores.append(np.median(cross_val_score(model_lr, X, y, cv=StratifiedKFold(y, ii, shuffle=False))))\n",
    "    print \"{}-fold cross-validation score: {}\".format(ii,scores[jj])\n",
    "    jj += 1\n",
    "print '\\n'\"Best CV score: {}\".format(max(scores))\n",
    "with plt.style.context('fivethirtyeight'):\n",
    "    plt.plot(range(2,11),scores)\n",
    "    plt.xlabel('K-fold')\n",
    "    plt.ylabel('Cross-val score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# find the CV score for 5-fold cross-validation\n",
    "cv_score_5fold = cross_val_score(model_lr,X,y,cv=5).mean()\n",
    "cv_score_5fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To obtain coefficients of features, we need to do a test train split, fit, predict, score and get coefs\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)\n",
    "lr_fitted = model_lr.fit(X_train,y_train)\n",
    "lr_fitted.score(X_test,y_test)\n",
    "(lr_fitted.coef_).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run KNN classifer\n",
    "myknn = KNeighborsClassifier(5).fit(X_train,y_train)\n",
    "\n",
    "# Output KNN score by running the model trained on training data on the test data\n",
    "myknn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run 5-fold cross-validation\n",
    "model = KNeighborsClassifier(5)\n",
    "data = X\n",
    "label = y\n",
    "cross_val_score(model, data, label, cv=StratifiedKFold(label, 5, shuffle=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit a Naive Bayes Model\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "preds = nb.predict(X_test)\n",
    "print metrics.accuracy_score(y_test, preds)\n",
    "print metrics.confusion_matrix(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions:**\n",
    "\n",
    "Using all features of df_sampled, all three models are performing poorly.\n",
    "- LR_accuracy: 35%\n",
    "- KNN_accruacy: 21%\n",
    "- Naive Bayes accuracy: 19%\n",
    "\n",
    "\n",
    "**Next steps:**\n",
    "\n",
    "- No point doing a Voting Classifier with above methods since individually all of them are bad.\n",
    "\n",
    "- Fit gradient boosting classifier and do a grid search\n",
    "\n",
    "- Synthetically upsample (SMOTE) response # 3,4 and downsample response 6,8 and see if performance of gradient boosting improves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "X = df_predictors_selected\n",
    "y = df_response_selected\n",
    "\n",
    "CV_FOLDS = 2\n",
    "\n",
    "# Set up grid search\n",
    "pgrid = {'learning_rate': [0.05, 0.1, 0.5], #'n_estimators': [1, 10, 100, 1000], \n",
    "         'subsample': [0.6, 0.75, 0.9],\n",
    "         'max_depth': [3,7,15]\n",
    "        }\n",
    "\n",
    "clf_grdbst = GradientBoostingClassifier(verbose=True)\n",
    "\n",
    "grid = GridSearchCV(clf_grdbst, param_grid=pgrid, cv = CV_FOLDS)\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid.best_estimator_.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient boosting classifier accuracy = 46.6%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
